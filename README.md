# Intern_Chatbot_Thesis_RAG

In this assignment I have developed a chatbot using Retrieval Augmented Generation (RAG) that answers questions related to my thesis. RAG is a technique that can be used to increase the correctness of answers in a specific domain. Moreover, it also decreases hallucinations. The RAG pipeline can be divided in two main steps. 

The first step of the RAG pipeline is indexing. Indexing is done through splitting the input, in the form of text, in a pre-determined number of subsets called chuncks. It is ensured that there is also is some amount of overlap present to make sure context is not lost. For example, if the text involves a mathematical theorem and proof, we want some overlap such that it is clear that these belong together. For each page in the document provided as input to the model a window of tokens is selected, with overlap, to be converted into embeddings. Embeddings are vector representations of words in a continuous vector space. Afterwards, each chunk, corresponding embeddings, and its metadeta (source, page) are uploaded to Azure AI Search.  

The second step of the RAG pipeline is retrieval and generation. For each user question the model retrieves data, this is done in a hybrid way. Azure uses a combination of semantics and keywords. For the semantics the query embedding is compared to the chunk embeddings, based on the vector similarity metric cosine a ranking is made. In parallel, keyword search is performed through classic BM25 lexical matching. Both results are combined using Reciprocal Rank Fusion (RRF). Then re-ranking is done through Cohere. The Cohere reranker merges each candidate chunk and the query together as a single input, then a relevance score is assigned using a transformer. This reads the full token interactions between query and chunk, giving higher precision on the final ordering. The top candidates are then selected and converted into a compact "Context" string. In this way, once a query is submitted to the large language model (LLM), the answer is grounded. Grounded means that the answer must come from the provided context only, no prior knowledge is included. If the the context is missing, the model will respond with something along the lines of "I don't know".

Furthermore, this chatbot has the following additional features: 
- Vanilla UI to ask questions and view clickable source pages from the thesis. The thesis is accessible through Azure blob storage. 
- Temperature of the Chat-GPT 4o-mini model is set to 0.1 such that answers are precise and concise. 
- FastAPI backend uses LangChain with Azure OpenAI to answer questions using retrieved context and trace responses
- Streaming of the responses is done via SSE.
-